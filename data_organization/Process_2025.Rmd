---
title: "Processing 2025 Data Pull from HydroVu with Auto Back-Calibration and QA/QC Flagging"
author: "Sam Struthers - CSU ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
---

# Script Purpose & Scope

This script is designed to pull in sensor data from HydroVu for the 2025 monitoring season, apply auto back-calibrations, export data for manual checks of back calibration issues, and run through the automated QA/QC flagging process. The final flagged dataset will be saved for manual review and verification.



## Setup Libraries 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading packages
package_loader <- function(x) {
  if (x %in% installed.packages()) {
    suppressMessages({
      library(x, character.only = TRUE)
    })
  } else {
    suppressMessages({
      install.packages(x)
      library(x, character.only = TRUE)
    })
  }
}

invisible(
  lapply(c("arrow",
           "data.table",
           "httr2",
           "tidyverse",
           "lubridate",
           "zoo",
           "padr",
           "stats",
           "RcppRoll",
           "yaml",
           "here",
           "ross.wq.tools",
           "furrr",
           "plotly"
  ),
  package_loader)
)

# Set up parallel processing
num_workers <- min(availableCores() - 1, 4) # Use at most 4 workers
plan(multisession, workers = num_workers)
set_furr_options <- furrr_options(
  globals = TRUE,
  packages = c("arrow", "data.table", "httr2", "tidyverse","dplyr", "lubridate", "zoo",
               "padr", "stats", "RcppRoll", "yaml", "here", "ross.wq.tools")
)

# suppress scientific notation to ensure consistent formatting
options(scipen = 999)

```

## Setting up the site/time range/parameters for the data pull

We deploy our sensors in March and pull them by the end of November. 
Always triple check the sites listed below to remove sites no longer deployed and add sites deployed this year. 
Double check the parameters of interest for spelling and inclusion of all needed parameters.

```{r}
#Adjust dates for specific pull
mst_start <- ymd_hms("2025-03-01 00:00:00", tz = "America/Denver")
mst_end <- ymd_hms("2025-11-30 23:59:59", tz = "America/Denver")

sites <- c(
  #Upper sites
  "joei", "cbri", "chd","pfal", "pbr", "sfm",  "pman", "pbd",
  #Lower sites
  "bellvue","salyer", "udall", "riverbend","cottonwood", "elc","archery","riverbluffs", 
  #lower tribs
  "boxcreek", "springcreek")

#Setting parameters of interest
parameters <- c("Specific Conductivity", "Temperature", "DO", "pH", "ORP", "Depth", #Standard vars
                "Chl-a Fluorescence", "Turbidity", "FDOM Fluorescence") #optical vars
```

## Establishing directory paths.

These are the folders where raw, and flagged data will be saved. Update base_folder to the current year and log directory as needed

```{r}
base_folder <- here("data","raw", "sensor","manual_data_verification","2025_cycle")
# Make sure base folder exists and create if not
if (!dir.exists(base_folder)) {
  dir.create(base_folder)
}

#folder with downloaded logs
log_directory <- here("data","raw", "sensor",  "log_download", "2025")  #Change as needed

#Sub folders for raw and flagged data
staging_directory <- here(base_folder,"hydro_vu_pull" ,"raw_data")
flagged_temp_dir <- here(base_folder,"hydro_vu_pull" ,"flagged_data_temp")
flagged_directory <- here(base_folder,"hydro_vu_pull" ,"flagged_data")

#check to make sure these all exist and create if needed
if (!dir.exists(staging_directory)) {
  dir.create(staging_directory, recursive = TRUE)
}
if (!dir.exists(flagged_temp_dir)) {
  dir.create(flagged_temp_dir, recursive = TRUE)
}
if (!dir.exists(flagged_directory)) {
  dir.create(flagged_directory, recursive = TRUE)
}

```

## Read in thresholds and credentials

Sensor thresholds are from In-SitU Inc spec sheets and seasonal_thresholds are created using `ross.wq.tools::make_threshold_table()` using cleaned data from previous years. 
mWater and HydroVu creds are created from their respective websites, contact Sam Struthers, Juan De La Torre or Katie Willi for help obtaining these.

```{r}
# Read in the threshold data first
sensor_thresholds <- read_yaml(here("data","derived","auto_qaqc_files", "thresholds",  "sensor_spec_thresholds.yml"))
season_thresholds <- read_csv(here("data","derived","auto_qaqc_files", "thresholds", "updated_seasonal_thresholds_2025_sjs.csv"), show_col_types = FALSE) %>%
  fix_site_names()

# Read in credentials
mWater_creds <- read_yaml(here("creds", "mWaterCreds.yml"))

hv_creds <- read_yaml(here("creds", "HydroVuCreds.yml"))
hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]))

```

# Data Pulls

Once these sections have been run and data have been checked using plots below, these sections should be commented out as they will take a long time to re-run. 

## Pulling in field notes from mWater
```{r}
# Pull in all notes
mWater_data <- load_mWater(creds = mWater_creds)
# Only extract sensor visits
all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data)%>%
  mutate(DT_round = with_tz(DT_round, tzone = "UTC"), # grab_mWater_sensor_notes converts this to MST, converting back to UTC for DS use
         last_site_visit = with_tz(last_site_visit, tzone = "UTC"),
         DT_join = as.character(DT_round))
# Only extract sensor malfunctions
sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data)%>%
  mutate(start_DT = with_tz(start_DT, tzone = "UTC"),# grab_mWater_malfunction_notes converts this to MST, converting back to UTC for DS use
         end_DT = with_tz(end_DT, tzone = "UTC"))
```

## Pulling in the sensor data from hydrovu

## HydroVu Pull

This section will take quite a while to load (~ 2-4 min per month of data per site). Once data has been pulled and checked, comment this section out to avoid re-running it.

```{r}
# # Get all the site info from HydroVu
# hv_sites <- hv_locations_all(hv_token)
# #trim down to just the sites/types of devices we want
# hv_sites <- hv_sites %>%
#   #Remove Vulinks
#   filter(!grepl("vulink", name, ignore.case = TRUE))%>%
#   #Remove Virridy sites/sondes
#   filter(!grepl("virridy", name, ignore.case = TRUE))#%>%
#    #remove any older sites (containing 2023 in the name)
#   filter(!grepl("2023", name, ignore.case = TRUE))
# 
# 
# walk(sites,
#      function(site) {
#        message("Requesting HV data for: ", site)
#        api_puller(
#          site = site,
#          start_dt = with_tz(mst_start, tzone = "UTC"),
#          end_dt = with_tz(mst_end, tzone = "UTC"),
#          api_token = hv_token,
#          hv_sites_arg = hv_sites,
#          dump_dir = staging_directory
#        )
#      }
# )
# 
# # beep to let you know when it's done
# beepr::beep(1)
```

## Pulling in any downloaded log data

Some logs (Vulink Log if sonde is AT600/800) cannot be backed up to HydroVu. If you have any of these files, we will read them in here so that we are not missing any potential data. Once this has been run and data checked, comment this section out to avoid re-running it.

```{r}
# log_files <- list.files(log_directory, pattern = "\\vulink.html$", full.names = TRUE)%>%
#   #only keep files with our sites in them
#   keep(~any(str_detect(tolower(.x), sites)))
# 
# log_files_simp <- log_files%>%
#   str_replace(log_directory, "")%>%
#   str_replace("/", "")%>%
#   str_replace_all("\\.html", "")%>%
#   str_squish()
# 
# log_data <- map(log_files, ~rvest::read_html(.x)%>%
#                             parse_insitu_html_log())
# 
# log_data <- map(html_content, ~parse_insitu_html_log(.x))
# 
# names(log_data) <- log_files_simp
# 
# cleaned_log_data <- log_data%>%
#   rbindlist()%>%
#   #fixing a few specific location names before passing to fix_site_names
#   mutate( site = tolower(site),
#           id = as.double(sensor_sn),
#          timestamp = with_tz(DT, tzone = "UTC"), 
#          site = case_when(site == "archery range"~ "archery", 
#                          site == "boxelder creek" ~ "boxcreek", 
#                          site == "spring creek" ~ "springcreek",
#                          TRUE ~ site), 
#          name = paste0("Vulink Log - ", site), 
#          #Correcting for parse log where temp is saved with degrees C rather than C
#          units = case_when(parameter == "Temperature" ~ "C", 
#                            TRUE ~ unit)
#          )%>%
#   fix_site_names()%>%
#   select(site, id, name, timestamp, parameter, value, units) %>%
#   distinct(.keep_all = TRUE) %>%
#   split(f = list(.$site), sep = "-") %>%
#   keep(~nrow(.) > 0)
# 
# #write out the log data to the staging directory so it can be processed with the HV data
# iwalk(cleaned_log_data, ~write_parquet(.x, here(staging_directory, paste0(.y, "_log_data.parquet"))))
# 
# # beep to let you know when it's done
# beepr::beep(1)

```


# Raw Data Processing

This is where we will read in all the pulled data from HydroVu and our parsed logs to then be tidied and checked.

## Load in and tidy the raw files

Here we use `munge_api_data()` and `tidy_api_data()` functions from `ross.wq.tools` to read in and tidy the raw data files. Our output will include lists seperated by site-parameter combinations for easier processing downstream. Each list has DT_round (in UTC), site, parameter, units, mean, n_obs, spread, DT_join and flag columns and is summarized to a 15 minute interval
```{r}
#Since we no longer have any virridy sites in the network, we can just use a normal munge!
hv_data <- munge_api_data(api_dir = staging_directory,synapse_env = F)%>%
  # split into site-parameter combinations for parallel processing
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)

tidy_data <- hv_data %>%
  future_map(~tidy_api_data(api_data = .),.progress = TRUE, .options = set_furr_options) %>%  # the summarize interval default is 15 minutes
  keep(~!is.null(.))%>%
  # Only keep parameters we are interested in
  keep(~unique(.$parameter) %in% parameters)

```
## Double check that pulls were successful

We are giving it a quick look over to make sure there aren't any large data gaps (>1 day) in the data pulled, that all site names are being applied correctly and that we don't have duplicate data. We are also adding in our field notes to the plots to make sure the data we have lines up with the periods we expect the sondes to be deployed and recording data. 

```{r}

data_check_plots <- tidy_data%>%
  bind_rows()%>%
  #Summarizing to an hour to speed up plotting
  mutate(DT_hourly = floor_date(DT_round, unit = "1 hour"))%>%
  group_by(site, parameter, DT_hourly)%>%
  summarize(hourly_med = median(mean,na.rm = T ))%>%
  split(f = list(.$site))%>%
  map(., function(df) {
    
    field_notes <- all_field_notes %>%filter(site == unique(df$site) & year(DT_round) == "2025")%>%
      select(DT_round,sonde_employed)%>%
      mutate(sonde_changed = case_when(sonde_employed == 0 ~ "Deployed",
                                       sonde_employed == 1 ~ "Pulled",
                                       TRUE ~ "Active"))
    
    ggplot(df, aes(x = DT_hourly, y = hourly_med)) +
      geom_point() +
      geom_vline(data = field_notes, aes(xintercept = DT_round, color = sonde_changed))+
      facet_wrap(~parameter, scales = "free_y")+
      labs(title = unique(df$site),
           x = "Date",
           y = "Hourly Median Value", 
           color = "Sonde Status")
  })
#Plot
walk(.x = data_check_plots, .f = print)

```

If there are discrepancies in the data, double check the raw files downloaded from HydroVu and the log files to make sure everything looks correct. Ocassionally there are issues with the HydroVu download missing data and that can be rerun for an individual site. If there continue to be large gaps in data (>3 days) during deployment periods, check field notes or contact Sam Struthers for troubleshooting.

# Apply Back-calibration as needed

This uses the Shiny App (https://github.com/juandlt-csu/manual_calibration_verification) developed by Juan. We are essentially deciding when a sensor needs to be back-calibrated based on field notes and applying the back-calibration to the raw data before flagging. The majority of calibrations are fine as is but we want to "skip" over the bad ones. 


## Read in calibration data

```{r}
#This creates calibration_data list object needed for the back-calibration functions
# load_calibration_data(
#   cal_data_file_path = here::here("data", "collated", "sensor", "cal_reports", "munged_calibration_data.RDS"),
#     update = FALSE, # set to TRUE when running for the first time
#   field_cal_dir = here::here("data", "raw", "sensor", "calibration_reports")
# )

```

## Organize sensor data to match formatting for `cal_join_sensor_calibration_data()`

Data should be in a nested list structure: `sensor_data$years$site-parameter`

```{r}
# sensor_data <- tidy_data %>%
#   bind_rows()%>%
#   split(f = year(.$DT_round)) %>%
#   map(~ {
#     split_data <- split(.x, f = list(.x$site, .x$parameter), sep = "-")
#     discard(split_data, ~is.null(.) || nrow(.) == 0)
#   })
```

## Join to calibration data and apply back-calibration

This workflow follows the workflow outlined in the `ross.wq.tools` vignette on back calibration. 
First, we join the calibration and sensor data so that each time instance has the slope and intercept info needed for back-calibration.
Next, we prepare the calibration windows to split the data into calibration windows. 
Lastly we loop through each calibration window to apply the back-calibration using `cal_back_calibrate()`.
```{r}

sensor_calibration_data <- cal_join_sensor_calibration_data(
  sensor_data_list = sensor_data,
  calibration_data_list = calibration_data
)

prepped_snsr_cal_data <- cal_prepare_calibration_windows(
  sensor_calibration_data_list = sensor_calibration_data
) 

calibrated_data <- prepped_snsr_cal_data%>%
  map(function(year){
    year%>%
      map(function(site_param){
        site_param %>% 
          map_dfr(function(chunk){
            cal_back_calibrate(chunk)
          })
      })
  })

beepr::beep(1)

```

## Save to back calibrated folder

This will become the "original" file in the shiny app for manual verification

```{r}

write_rds(calibrated_data, here("data", "raw", "sensor", "manual_data_verification", "2025_cycle","hydro_vu_pull","back_calibration", "calibrated_sensor_data_2025.rds"))

```

Take ^ file and run through the shiny app to verify back-calibration and make any manual adjustments as needed. 

## Read in user corrected back-calibrated data

```{r}
calibrated_data_final <- read_rds(here("data", "raw", "sensor", "manual_data_verification", "2025_cycle","hydro_vu_pull","back_calibration", "calibrated_sensor_field_data_finalized_2025.rds"))[["2025"]] # Only extract the 2025 data

```

## Quick look at the back-calibrated data

All of this should already have been double checked by the user but we just want to make sure everything looks good before proceeding to flagging. This plot will show the raw vs back-calibrated data ("final") with vertical lines indicating when the calibration events occurred.

```{r}

plots <- map(names(calibrated_data_final), 
             function(site_param) {
               plot_data <- calibrated_data_final[[site_param]]
               
               vline_df <- plot_data %>% 
                 group_by(sensor_date) %>% 
                 slice_min(DT_round, n = 1) %>%
                 arrange(DT_round)%>%
                 ungroup()%>%
                 select(DT_round, file_date)%>%
                 mutate(file_date_mst = with_tz(file_date, tzone = "MST"), 
                        file_date_mst_utc = force_tz(file_date_mst, tzone = "UTC"))
               
               param <- unique(plot_data$parameter)
               site <- unique(plot_data$site)
               
               ggplotly(ggplot(plot_data, aes(DT_round))+
                          geom_line(aes(y = mean, color = "raw"))+
                          geom_line(aes(y = mean_cal, color = "final"))+
                          geom_vline(xintercept = vline_df$file_date_mst_utc, linetype = "dashed")+
                          scale_color_manual(name = "Data Type", values = c("raw" = "blue", "final" = "green4"))+
                          labs(title = paste0(site, " - ", param),
                               x = "Date",
                               y = param))
             })%>%
  set_names(names(calibrated_data_final))

#Typically easiest to go parameter-wise through the plots 
plots[grepl("pH", names(plots))]

```

## Remove columns as needed

Here we are removing columns that are not needed downstream to reduce memory usage. We are also merging back in the units, n_obs, spread and flag columns from the original tidy data. Temperature and Depth are not passed into the back-calibration process so we will grab those from the original tidy data as well. We will also add the column "back_cal_performed" to indicate whether a back-calibration was actually applied to the data or not.

```{r}

cal_corrected_data <- calibrated_data_final%>%
  map(., function(df){
    
    site <- unique(df$site)
    parameter <- unique(df$parameter)
    
    site_param = paste0(site, "-", parameter)
    
    add_data <- tidy_data[[site_param]]%>%
      select( DT_join, units, n_obs, spread, flag)
    
    if("updated_slope_from" %in% names(df)){
      df <- df%>%
        mutate(back_cal_performed = if_else(slope == updated_slope_to & slope == updated_slope_from, F, T))
    } else {
      df <- df%>%
        mutate(back_cal_performed = F)
    }
    
    df%>%
      select(
        DT_round,
        site,
        parameter,
        mean_pre_cal = mean,
        mean = mean_cal,
        back_cal_performed
      )%>%
      mutate(DT_join = as.character(DT_round))%>%
      left_join(add_data, by = c("DT_join"))
  })

# We will also need to grab the Depth and Temperature data (not back calibrated) and merge it here
non_cal_corrected_data <- tidy_data%>%
  keep(~unique(.$parameter) %in% c("Depth", "Temperature"))%>%
  map(., function(df){
    df%>%
      mutate(back_cal_performed = FALSE,
             mean_pre_cal = mean)%>%
      select(,
             DT_round,
             site,
             parameter,
             mean_pre_cal,
             mean,
             back_cal_performed,
             DT_join,
             units,
             n_obs,
             spread,
             flag
      )
  })

# Bind all data together
tidy_cal_data <- c(cal_corrected_data, non_cal_corrected_data)

```

# Data Flagging

## Combine with field notes
Here we read in our field notes and then join them to the calibrated data using `add_field_notes()`. We then generate summary statistics (slope, lag, etc) using `generate_summary_statistics()`.
```{r}

# Add the field note data to all of the data
all_field_notes <- all_field_notes %>%
  fix_site_names()

combined_data <- tidy_cal_data %>%
  future_map(~add_field_notes(df = ., notes = all_field_notes), .progress = TRUE)

# Add summary statistics
summarized_data <- combined_data %>%
  map(~generate_summary_statistics(.))

```

## Single Sensor Flags

This is the first round of flagging that is done on a per-site-parameter basis. Here we are applying functions to flag field visits, missing data, DO noise, repeating values, depth shifts, optical sensor drift, spec range violations and seasonal range violations. This can take ~5-15 min depending on data volume.

```{r}
# Chunk the data for furrr
summarized_data_chunks <- split(1:length(summarized_data),
                                ceiling(seq_along(1:length(summarized_data))/10))
# Flag data...
# Single parameter flags
single_sensor_flags <- list()
for (chunk_idx in seq_along(summarized_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(summarized_data_chunks), " ===")
  
  # Get the indices for this chunk
  indices <- summarized_data_chunks[[chunk_idx]]
  chunk_data <- summarized_data[indices]
  
  # Process the chunk in parallel
  chunk_results <- chunk_data %>%
    future_map(
      function(data) {
        flagged_data <- data %>%
          data.table(.) %>%
          # flag field visits
          add_field_flag(df = .) %>%
          # flag missing data
          add_na_flag(df = .) %>%
          # flag DO noise
          find_do_noise(df = .) %>%
          # flag repeating values
          add_repeat_flag(df = .) %>%
          # find times when sonde was moved up/down in housing
          add_depth_shift_flag(df = ., level_shift_table =  all_field_notes, post2024 = TRUE) %>%
          # find instances of sensor drift (FDOM, Chl-a, Turbidity only)
          add_drift_flag(df = .)
        
        if (unique(data$parameter) %in% names(sensor_thresholds)) {
          # flag instances outside the spec range
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_spec_flag(df = ., spec_table = sensor_thresholds)
        }
        
        if (unique(data$parameter) %in% unique(season_thresholds$parameter)) {
          # flag instances outside the spec range
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_seasonal_flag(df = ., threshold_table = season_thresholds)
        }
        
        flagged_data <- flagged_data %>%
          data.table(.)
        
        return(flagged_data)
      },
      .progress = TRUE, .options = set_furr_options
    )
  
  # Add chunk to list
  single_sensor_flags <- c(single_sensor_flags, chunk_results)
  
  if (chunk_idx < length(summarized_data_chunks)) {
    message("Taking a short break before next chunk...")
    gc()
    Sys.sleep(0.1)
  }
}

# beep to let you know when it's done
beepr::beep(1)

```

## Intrasensor flags

Now we will apply flags that require multiple parameters from the same sensor to be present. This includes frozen water flagging (temp < 0C), unsubmerged flagging (depth < 0 or SC < 10) and sonde burial flagging (long-term DO noise). We also do an intersensor check to remove slope violation flags that occur concurrently with temp or depth violations to reduce overflagging. This can take ~5-15 min depending on data volume.

```{r}

intrasensor_flags <- single_sensor_flags %>%
  rbindlist(fill = TRUE) %>%
  split(by = "site")

# Chunk the data for furrr
intrasensor_data_chunks <- split(1:length(intrasensor_flags),
                                 ceiling(seq_along(1:length(intrasensor_flags))/3))

intrasensor_flags_list <- list()
for (chunk_idx in seq_along(intrasensor_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(intrasensor_data_chunks), " ===")
  
  # Get the indices for this chunk
  indices <- intrasensor_data_chunks[[chunk_idx]]
  chunk_data <- intrasensor_flags[indices]
  # Process the chunk in parallel
  chunk_results <- chunk_data %>%
    future_map(
      function(data) {
        # A chunk is a site df
        flagged_data <- data %>%
          data.table() %>%
          # flag times when water was below freezing
          add_frozen_flag(.) %>%
          # overflagging correction. remove slope violation flag if it occurs concurrently
          # with temp or depth
          intersensor_check(.) %>%
          # add sonde burial. If DO is noise is long-term, likely burial:
          add_burial_flag(.) %>%
          # flag times when sonde was unsubmerged
          add_unsubmerged_flag(.)
        
        return(flagged_data)
      }, .progress = TRUE, .options = set_furr_options
    ) %>%
    rbindlist(fill = TRUE) %>%
    # lil' cleanup of flag column contents
    dplyr::mutate(flag = ifelse(flag == "", NA, flag)) %>%
    distinct(site, parameter, DT_round, mean, .keep_all = TRUE) %>%
    # transform back to site-parameter dfs
    split(f = list(.$site, .$parameter), sep = "-") %>%
    purrr::discard(~ nrow(.) == 0) %>%
    # Add in KNOWN instances of sensor malfunction
    map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))
  
  # Add chunk to list
  intrasensor_flags_list <- c(intrasensor_flags_list, chunk_results)
  
  if (chunk_idx < length(intrasensor_data_chunks)) {
    message("Taking a short break before next chunk...")
    gc()
    Sys.sleep(0.1)
  }
}

# beep to let you know when it's done
beepr::beep(1)

```

### Let's temporarily save this data so we can remove everything else from memory

```{r}
iwalk(intrasensor_flags_list, ~write_parquet(.x, here(flagged_temp_dir, paste0(.y, ".parquet"))))

#remove everything other than intrasensor flags from memory
rm(list=setdiff(ls(), c("base_folder", "intrasensor_flags_list", "flagged_directory")))
#clean up memory
gc()

```

## Applying network check

The network check function `ross.wq.tools::network_check()` compares data across sites to identify when data is being overflagged and remove flags as needed. This step requires knowledge of the site order, which is read in from a YAML file and should be updated as sites change.

```{r}
#Read in site order for year/pull
site_order_2025 <- load_site_order( here("data","derived","auto_qaqc_files", "site_order", "site_order_2025.yml"))

#Applying Network check (non parrallel so only map used)
final_flags <- intrasensor_flags_list %>%
  map(~network_check(df = ., intrasensor_flags_arg = intrasensor_flags_list, site_order_arg = site_order_2025)) %>%
  rbindlist(fill = TRUE) %>%
  tidy_flag_column() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  map(~add_suspect_flag(.)) %>%
  rbindlist(fill = TRUE)

v_final_flags <- final_flags%>%
  mutate(auto_flag = ifelse(is.na(auto_flag), NA,
                            ifelse(auto_flag == "suspect data" & is.na(lag(auto_flag, 1)) & is.na(lead(auto_flag, 1)), NA, auto_flag))) %>%
  select(c("DT_round", "DT_join", "site", "parameter","mean_pre_cal", "mean", "units", "n_obs", "spread", "auto_flag", "mal_flag", "sonde_moved","sonde_employed", "season", "last_site_visit", "depth_change", "back_cal_performed")) %>%
  mutate(auto_flag = ifelse(is.na(auto_flag), NA, ifelse(auto_flag == "", NA, auto_flag))) %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)

# beep to let you know when it's done
beepr::beep(1)

```

## Check final output

Here we are making sure that all sites data were flagged and giving a quick look at the flagged data to make sure everything looks correct. With newer sites/parameters, we expect there to be over-flagging as the seasonal ranges/slopes are going to be very small. 

```{r}
v_final_flags%>%
  bind_rows()%>%
  split(f = list(.$site))%>%
  map(., function(df) {
    ggplot(df, aes(x = DT_round, y = mean, color = auto_flag)) +
      geom_point() +
      facet_wrap(~parameter, scales = "free_y")+
      labs(title = unique(df$site),
           x = "Date",
           y = "Mean Value")
  }) %>%
  walk(plot)
```


# Save Final Flagged Dataset

We will save it into two places: 
- Individually flagged site-parameter parquet files for posterity and comparison with manual verification to determine effectiveness of auto-flagging
- A combined raw data parquet file for use in the manual verification shiny app. Once starting manual verification for the year, this will be used the as "uploaded" data to generate all the necessary files for the app. 

```{r}

# Save the data individually.
iwalk(v_final_flags, ~write_parquet(.x, here(flagged_directory,  paste0(.y, ".parquet"))))

#saving our data into the "raw_data" folder of the manual verification folder for use in the shiny app
write_parquet(v_final_flags%>%bind_rows(), here(base_folder,"in_progress","raw_data", "raw_data_2025.parquet"))s
```



