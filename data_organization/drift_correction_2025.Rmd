---
title: "drift_correction_2025"
output: html_document
author: "Juan De La Torre"
---

The objective of this notebook is to manually correct the drift in the turbidity
and FDOM sensors for 2025. I will try to document as much of the work and reasoning
here as possible. Note that this workflow is not meant to be reproducible.

Some notes on the work so far. 

The workflow itself is fine. I think that the 
functions work and make sense, but they are still rough around the edges and could
be difficult to use in a general way. I think that they unflagging function is 
incomplete, it needs to take care of more things than just the 
`user_flag` column. Right now I defaulted to correcting all exponential drift
instances with a log solution, but I think that there are still cases where the 
exponential solution could work better. There is definitely still some things 
that I would like to explore around that point. Overall, I think that these functions work
as intended. They are not really deciding anything, so I think that a lot of the 
"weird" results from this correction are due to mislabeling the drift type as linear instead of 
exponential and vice versa. For Turbidity the correction method was almost always
multiplicative, and I think that it worked out real nice for taming extreme variability.
For FDOM the drift type was always linear and the correction method was almost always additive. 

Drift detection and labeling is difficult. Real time drift detection is more so. 
I think we can think of some clever, novel solutions to those problems, but for 
now the manual drift labeling worked best for this workflow. This meant that I 
had an excel sheet open to mark down the start and end time for drift, as well
as what I thought would make sense for the drift instance's type and correction
method. This was slow work, but not nearly as hard as I thought it might be. The 
data that I was using had already been manually verified, so that helped a lot.
There were still instances of drift that I needed to correct, add, or remove. You 
can view these decisions in the `decision_df` and the actual excel sheet is in 
`here("data", "derived", "auto_qaqc_files", "drift_corrections", "2025_drift_corrections.xlsx")`.

This workflow uses some functions from `ross.wq.tools` and the uCLP work that I 
needed to tweak in order to make them work for this context.

I also had Claude make a couple of plotting functions to explore the data. I think
that for a broad review you could largely ignore those, run everything, and jump to 
the bottom of the script to look at the corrected data. It is useful to pull up
`decision_df` to look at the decisions that were made and how well they line up
with the final corrected data. The final column that has all of the corrected data 
is `mean_filled`. 

The actual correction functions were developed only considering each specific
situation that needed a correction function. In actuality, these functions are
all special cases of each other and have a lot of the same bones. These could all
probably get joined into one huge function later, but for review this is easier 
to read. They get orchestrated in the `# The whole workflow orchestrated` section,
but you can see what each function is trying to do before then. 

Overall, I am feeling pretty good about this workflow so far. There is still a 
fair amount of work left to make this a true programmatic correction workflow, 
but we can at least use this to correct the 2025 data. I will note that I checked
all of the drift data that was corrected to make sure that it looked good, but a 
double check on those would be a good idea. 

Some notes on future work:
- Automatic drift detection method needs more work
- Automatic correction decision method needs to be made.
  - Right now we manually decide what to do with the drift that we see. For 
  FDOM this could be handled pretty easily by automatically setting those corrections
  to linear and additive. For Turbidity there needs to be some rules to determine 
  if a correction should be exponential or linear, but they should always default
  to multiplicative. 
- The decision dataframe has some decisions that could be updated after this work.
- There is a need to explore a true exponential solution. 
  - While working on the exponential solution a kept running into "humps" after
  having corrected the data. Sometimes a linear solution would provide better 
  results given a shorter duration and magnitude of the drift, but when the drift
  was very exponential linear solutions still gave me humps. Then I realized that
  to fix this I needed a log solution. I could fit an exponential curve to the 
  data, but if the curve was tame enough we actually wanted stronger correction 
  closer to the tail end of the correction for longer, so that means an inverse
  of the exponential shape. This doesn't work when the curve is very, very strong.
  Though the log solution will bring the data down it will, in my opinion, overcorrect.
  I think that those are situations where a true exponential solution should work
  best, those situations where the drop off in correction should be quick. That
  means that there needs to be a "true" `exponential_correction_fxn` and there needs to
  be an update to the current one. This also means that the decision df needs to 
  be updated as well, so that we can differentiate between "exponential" and "log"
  solutions.
  

# Environment set up

```{r}
knitr::opts_chunk$set(
  eval = FALSE,
  echo = TRUE
)
```

```{r}
#Installing and loading all packages
invisible(
  lapply(c(
    "tidyverse", # Data manipulation
    "lubridate", # Date-Time Manipulation
    "readxl", # Reading excel files
    "here", # Easy, cross platform file referencing
    "ggplot2", # Plotting libraries
    "patchwork",
    "ggpubr",
    "plotly",
    "devtools", # For downloading GitHub packages
    "remotes",
    "arrow", # Reading parquet files
    "groupdata2",
    "ross.wq.tools" # In house functions
  ),
  function(x) {
    if (x %in% installed.packages()) {
      suppressMessages({
        library(x, character.only = TRUE)
      })
    } else {
      suppressMessages({
        install.packages(x)
        library(x, character.only = TRUE)
      })
    }
  })
)
```

## Functions from ross.wq.tools

Local copies of `ross.wq.tools` functions used in this workflow. The version of
`cal_exp_one_point_drift` below extends the package version with an `exp_coef`
parameter so that a fitted `k` can be supplied for exponential corrections.

```{r}
#' @title Add a column to a data frame if it does not exist

add_column_if_not_exists <- function(df, column_name, default_value = NA) {
  if (!column_name %in% colnames(df)) {
    df <- df %>% dplyr::mutate(!!sym(column_name) := default_value)
  }
  return(df)
}
```

```{r}
#' @title Temporal Weight Factor Calculation

cal_wt <- function(df, dt_col, wt_col = "wt"){
  
  if(!dt_col %in% colnames(df)){
    stop(paste0("The specified dt_col '", dt_col, "' is not present in the dataframe."))
  }
  
  first_dt <- min(df[[dt_col]])
  last_dt  <- max(df[[dt_col]])
  tot_time <- as.numeric(difftime(last_dt, first_dt, units = "mins"))
  
  df <- df %>%
    dplyr::mutate(!!wt_col := as.numeric(difftime(.data[[dt_col]], first_dt, units = "mins")) / tot_time)
  
  return(df)
}
```

```{r}
#' @title Exponential One Point Drift Calibration (local — adds exp_coef param)
#'
#' @param df Tibble containing sensor data bounded by two calibrations
#' @param lm_trans_col Column name containing linearly transformed data
#' @param pre_col Column name for pre-cleaning measurement
#' @param post_col Column name for post-cleaning measurement
#' @param wt_col Column name containing temporal weight parameters
#' @param drift_type "linear" (default), "exponential", or "log"
#' @param correction_type "additive" (default) or "multiplicative"
#' @param exp_coef Rate parameter k (default 1). Supply a fitted k from exp_k().
#'   For "log", the weight is the inverse of the exponential curve with the same k:
#'   wt_log = log(1 + wt * (exp(k) - 1)) / k

cal_exp_one_point_drift <- function(df, lm_trans_col, pre_col, post_col, wt_col,
                                    drift_type = "linear", correction_type = "additive",
                                    exp_coef = 1){
  
  transformed_col <- paste0(str_split_1(lm_trans_col, "_")[1], "_drift_trans")
  
  pre_drift_back_calibration  <- df[[pre_col]][[nrow(df)]]
  post_drift_back_calibration <- df[[post_col]][[nrow(df)]]
  
  if (is.na(pre_drift_back_calibration) || is.na(post_drift_back_calibration)) {
    df <- df %>%
      dplyr::mutate(!!transformed_col := NA_integer_)
    return(df)
  }
  
  if (drift_type == "exponential") {
    df <- df %>%
      dplyr::mutate(wt_exp = (exp(exp_coef * .data[[wt_col]]) - 1) / (exp(exp_coef) - 1))
    wt_col_to_use <- "wt_exp"
  } else if (drift_type == "log") {
    df <- df %>%
      dplyr::mutate(wt_log = log(1 + .data[[wt_col]] * (exp(exp_coef) - 1)) / exp_coef)
    wt_col_to_use <- "wt_log"
  } else {
    wt_col_to_use <- wt_col
  }
  
  if (correction_type == "multiplicative") {
    drift_ratio <- post_drift_back_calibration / pre_drift_back_calibration
    df <- df %>%
      dplyr::mutate(!!transformed_col := .data[[lm_trans_col]] * (1 - .data[[wt_col_to_use]] + .data[[wt_col_to_use]] * drift_ratio))
  } else {
    standard_delta <- post_drift_back_calibration - pre_drift_back_calibration
    df <- df %>%
      dplyr::mutate(!!transformed_col := .data[[lm_trans_col]] + (.data[[wt_col_to_use]] * standard_delta))
  }
  
  return(df)
}
```

```{r}
#' @title Low-pass filter for turbidity noise reduction
#' @description Applies a 5-point binomial kernel three times in succession to
#'   smooth optical noise. Only applied to Turbidity data.

low_pass_filter <- function(df, correction_col = "mean_drift_trans") {
  
  if (unique(df$parameter) == "Turbidity") {
    
    binomial_kernel <- function(int_vec) {
      kernel <- c(1, 4, 6, 4, 1)
      x <- sum(int_vec * kernel) / 16
      return(x)
    }
    
    df <- df %>%
      add_column_if_not_exists(column_name = "smoothed_mean") %>%
      mutate(
        smoothed_mean = if_else(is.na(smoothed_mean),
                                data.table::frollapply(!!sym(correction_col), N = 5, FUN = binomial_kernel,
                                                       fill = NA, align = "center"),
                                smoothed_mean),
        smoothed_mean = if_else(is.na(smoothed_mean),
                                data.table::frollapply(smoothed_mean, N = 5, FUN = binomial_kernel,
                                                       fill = NA, align = "center"),
                                smoothed_mean),
        smoothed_mean = if_else(is.na(smoothed_mean),
                                data.table::frollapply(smoothed_mean, N = 5, FUN = binomial_kernel,
                                                       fill = NA, align = "center"),
                                smoothed_mean)
      )
    
    return(df)
  } else {
    df <- df %>% mutate(smoothed_mean = .data[[correction_col]])
    return(df)
  }
}
```

```{r}
#' @title Interpolate missing data within a maximum gap
#' @description Fills NA values in the value column using linear or spline
#'   interpolation, up to max_gap consecutive missing observations.

apply_interpolation_missing_data <- function(df, site_col = "site", parameter_col = "parameter",
                                             dt_col = "DT_round", value_col = "smoothed_mean",
                                             max_gap = 4, new_value_col = "mean_filled",
                                             method = "linear") {
  
  dt_vals <- df[[dt_col]]
  if (!lubridate::is.POSIXt(dt_vals)) {
    stop("Error: The datetime column must be POSIXct/POSIXt.")
  }
  
  pad_check <- df %>%
    dplyr::group_by(dplyr::across(all_of(c(site_col, parameter_col)))) %>%
    dplyr::group_walk(~ {
      dt_vals      <- sort(unique(.x[[dt_col]]))
      dt_diffs     <- diff(dt_vals)
      unique_diffs <- unique(dt_diffs)
      if (length(unique_diffs) > 1) {
        warning(
          paste0("Error: Datetime column is not equally spaced for site = ",
                 unique(.y[[site_col]]), ", parameter = ", unique(.y[[parameter_col]]),
                 ". Please pad with padr::pad() first.")
        )
      }
    })
  
  `%nin%` = Negate(`%in%`)
  
  if(method %nin% c("spline", "linear")){
    stop("method must be either 'spline' or 'linear'")
  }
  
  if(method == "linear") {
    return(
      df %>%
        arrange(!!sym(site_col), !!sym(parameter_col), !!sym(dt_col)) %>%
        group_by(!!sym(site_col), !!sym(parameter_col)) %>%
        mutate(
          !!sym(new_value_col) := zoo::na.approx(
            x      = as.numeric(!!sym(dt_col)),
            object = !!sym(value_col),
            maxgap = max_gap,
            na.rm  = FALSE
          )
        ) %>%
        ungroup()
    )
  }
  
  if(method == "spline"){
    return(
      df %>%
        arrange(!!sym(site_col), !!sym(parameter_col), !!sym(dt_col)) %>%
        group_by(!!sym(site_col), !!sym(parameter_col)) %>%
        mutate(
          !!sym(new_value_col) := zoo::na.spline(
            x      = as.numeric(!!sym(dt_col)),
            object = !!sym(value_col),
            maxgap = max_gap,
            na.rm  = FALSE
          )
        ) %>%
        ungroup()
    )
  }
}
```

# Plotting functions

```{r}
# Template to find downstream and upstream data
site_order <- list(
  clp = c("joei", "cbri", "chd", "pfal", "pbr", "pman", "pbd", "bellvue",
          "salyer", "udall", "riverbend", "cottonwood", "elc", "archery", "riverbluffs"),
  springcreek = c("riverbend", "springcreek", "cottonwood"),
  boxcreek = c("elc", "boxcreek", "archery"),
  sfm = c("sfm"),
  mtn_campus = c("mtn_campus")
)
```

```{r}
#' @title Plot drift flags with network context

plot_drift <- function(df, data_list, site_order_template) {
  
  site_name      <- unique(na.omit(df$site))
  parameter_name <- unique(na.omit(df$parameter))
  
  matching_networks <- keep(site_order_template, ~ site_name %in% .x)
  
  above_df <- NULL
  below_df <- NULL
  
  if (length(matching_networks) > 0) {
    if (site_name %in% names(matching_networks)) {
      sites_order <- matching_networks[[site_name]]
    } else {
      sites_order <- matching_networks[[1]]
    }
    
    site_index <- which(sites_order == site_name)
    
    if (site_index > 1) {
      above_key <- paste0(sites_order[site_index - 1], "-", parameter_name)
      if (above_key %in% names(data_list)) above_df <- data_list[[above_key]]
    }
    
    if (site_index < length(sites_order)) {
      below_key <- paste0(sites_order[site_index + 1], "-", parameter_name)
      if (below_key %in% names(data_list)) below_df <- data_list[[below_key]]
    }
  }
  
  plot_title <- paste(site_name, parameter_name, sep = " ")
  
  site_visits <- df %>%
    filter(str_detect(flag, "site visit") & !str_detect(flag, "site visit window")) %>%
    pull(DT_round)
  
  vlines <- lapply(site_visits, function(sv) {
    list(type = "line",
         x0 = sv, x1 = sv,
         y0 = 0, y1 = 1, yref = "paper",
         line = list(color = "grey", dash = "dash"))
  })
  
  p <- plot_ly() %>%
    add_trace(data = df,
              x = ~DT_round,
              y = ~mean_analysis,
              color = ~drift,
              colors = c("FALSE" = "steelblue", "TRUE" = "tomato"),
              type = "scatter",
              mode = "markers",
              legendgroup = "active")
  
  if (!is.null(above_df)) {
    p <- p %>%
      add_trace(data = above_df,
                x = ~DT_round,
                y = ~mean_analysis,
                type = "scatter",
                mode = "markers",
                marker = list(color = "rgba(150, 150, 150, 0.5)", symbol = "triangle-up", size = 4),
                name = paste0(unique(above_df$site), " (above)"),
                legendgroup = "above")
  }
  
  if (!is.null(below_df)) {
    p <- p %>%
      add_trace(data = below_df,
                x = ~DT_round,
                y = ~mean_analysis,
                type = "scatter",
                mode = "markers",
                marker = list(color = "rgba(150, 150, 150, 0.5)", symbol = "triangle-down", size = 4),
                name = paste0(unique(below_df$site), " (below)"),
                legendgroup = "below")
  }
  
  p %>%
    layout(title = plot_title,
           xaxis = list(title = "Date"),
           yaxis = list(title = "Mean Analysis"),
           shapes = vlines)
}
```

```{r}
#' @title Plot drift correction results with network context

plot_correct_drift <- function(df, data_list, site_order_template) {
  
  site_name      <- unique(na.omit(df$site))
  parameter_name <- unique(na.omit(df$parameter))
  plot_title     <- paste(site_name, parameter_name, sep = " ")
  
  matching_networks <- keep(site_order_template, ~ site_name %in% .x)
  
  above_df <- NULL
  below_df <- NULL
  
  if (length(matching_networks) > 0) {
    if (site_name %in% names(matching_networks)) {
      sites_order <- matching_networks[[site_name]]
    } else {
      sites_order <- matching_networks[[1]]
    }
    
    site_index <- which(sites_order == site_name)
    
    if (site_index > 1) {
      above_key <- paste0(sites_order[site_index - 1], "-", parameter_name)
      if (above_key %in% names(data_list)) above_df <- data_list[[above_key]]
    }
    
    if (site_index < length(sites_order)) {
      below_key <- paste0(sites_order[site_index + 1], "-", parameter_name)
      if (below_key %in% names(data_list)) below_df <- data_list[[below_key]]
    }
  }
  
  p <- plot_ly()
  
  if (!is.null(above_df)) {
    p <- p %>%
      add_trace(data = above_df,
                x = ~DT_round,
                y = ~mean_analysis,
                type = "scatter",
                mode = "markers",
                marker = list(color = "rgba(150, 150, 150, 0.5)", symbol = "triangle-up", size = 4),
                name = paste0(unique(above_df$site), " (above)"),
                legendgroup = "above")
  }
  
  if (!is.null(below_df)) {
    p <- p %>%
      add_trace(data = below_df,
                x = ~DT_round,
                y = ~mean_analysis,
                type = "scatter",
                mode = "markers",
                marker = list(color = "rgba(150, 150, 150, 0.5)", symbol = "triangle-down", size = 4),
                name = paste0(unique(below_df$site), " (below)"),
                legendgroup = "below")
  }
  
  p %>%
    add_trace(data = df,
              x = ~DT_round,
              y = ~mean_analysis,
              type = "scatter",
              mode = "markers",
              marker = list(color = "tomato", size = 4),
              name = "Original") %>%
    add_trace(data = df,
              x = ~DT_round,
              y = ~mean_filled,
              type = "scatter",
              mode = "markers",
              marker = list(color = "seagreen", size = 4),
              name = "Filled") %>%
    layout(title = plot_title,
           xaxis = list(title = "Date"),
           yaxis = list(title = parameter_name))
}
```

```{r}
#' @title Plot final corrected data with correction type highlighted
#' @description Plots the corrected time series coloured by how each segment
#'   was treated: raw (no correction), linear, exponential (log), or unflagged.
#'   Upstream/downstream neighbours are shown in grey for network context.

plot_final_corrections <- function(df, data_list, site_order_template) {

  site_name      <- unique(na.omit(df$site))
  parameter_name <- unique(na.omit(df$parameter))
  plot_title     <- paste(site_name, parameter_name, "- Final Corrections")

  # Upstream / downstream neighbours
  matching_networks <- keep(site_order_template, ~ site_name %in% .x)

  above_df <- NULL
  below_df <- NULL

  if (length(matching_networks) > 0) {
    if (site_name %in% names(matching_networks)) {
      sites_order <- matching_networks[[site_name]]
    } else {
      sites_order <- matching_networks[[1]]
    }

    site_index <- which(sites_order == site_name)

    if (site_index > 1) {
      above_key <- paste0(sites_order[site_index - 1], "-", parameter_name)
      if (above_key %in% names(data_list)) above_df <- data_list[[above_key]]
    }

    if (site_index < length(sites_order)) {
      below_key <- paste0(sites_order[site_index + 1], "-", parameter_name)
      if (below_key %in% names(data_list)) below_df <- data_list[[below_key]]
    }
  }

  # correction_type is stamped by the orchestrator — use it directly
  plot_df <- df

  # Build plot — neighbours first so they sit behind the main traces
  p <- plot_ly()

  if (!is.null(above_df)) {
    p <- p %>%
      add_trace(data = above_df,
                x = ~DT_round, y = ~mean_analysis,
                type = "scatter", mode = "markers",
                marker = list(color = "rgba(150, 150, 150, 0.25)", symbol = "triangle-up", size = 7),
                name = paste0(unique(above_df$site), " (above)"),
                legendgroup = "above")
  }

  if (!is.null(below_df)) {
    p <- p %>%
      add_trace(data = below_df,
                x = ~DT_round, y = ~mean_analysis,
                type = "scatter", mode = "markers",
                marker = list(color = "rgba(150, 150, 150, 0.25)", symbol = "triangle-down", size = 7),
                name = paste0(unique(below_df$site), " (below)"),
                legendgroup = "below")
  }

  # Raw mean_analysis in red with light alpha — visible behind everything as the
  # "corrected from" reference for all points
  p <- p %>%
    add_trace(data = plot_df,
              x = ~DT_round, y = ~mean_analysis,
              type = "scatter", mode = "markers",
              marker = list(color = "rgba(255, 0, 0, 0.15)", size = 4),
              name = "raw")

  # Final smoothed/interpolated values for uncorrected rows in black
  raw_df <- plot_df %>% filter(correction_type == "raw")

  p <- p %>%
    add_trace(data = raw_df,
              x = ~DT_round, y = ~mean_filled,
              type = "scatter", mode = "markers",
              marker = list(color = "black", size = 4),
              name = "no correction")

  # Corrected values overlaid in colour — only for rows that were actually changed
  corrected_df <- plot_df %>% filter(correction_type != "raw")

  p %>%
    add_trace(data = corrected_df,
              x = ~DT_round, y = ~mean_filled,
              color = ~correction_type,
              colors = c(
                "unflagged"   = "seagreen",
                "linear"      = "steelblue",
                "exponential" = "mediumpurple"
              ),
              type = "scatter", mode = "markers",
              marker = list(size = 4)) %>%
    layout(title = plot_title,
           xaxis = list(title = "Date"),
           yaxis = list(title = parameter_name))
}
```

```{r}
#' @title Plot before/after unflag comparison

plot_unflag_comparison <- function(original_df, unflagged_df) {
  
  site_name      <- unique(na.omit(original_df$site))
  parameter_name <- unique(na.omit(original_df$parameter))
  plot_title     <- paste(site_name, parameter_name, "- Unflag Comparison")
  
  comparison_df <- original_df %>%
    select(DT_round, mean_analysis, drift_before = drift) %>%
    left_join(
      unflagged_df %>% select(DT_round, drift_after = drift),
      by = "DT_round"
    ) %>%
    mutate(
      status = case_when(
        !drift_before               ~ "never flagged",
        drift_before & !drift_after ~ "unflagged",
        drift_after                 ~ "still flagged"
      )
    )
  
  plot_ly(data = comparison_df,
          x = ~DT_round,
          y = ~mean_analysis,
          color = ~status,
          colors = c("never flagged" = "grey",
                     "unflagged"     = "seagreen",
                     "still flagged" = "tomato"),
          type = "scatter",
          mode = "markers") %>%
    layout(title = plot_title,
           xaxis = list(title = "Date"),
           yaxis = list(title = parameter_name))
}
```

# Correction functions

```{r}
#' @title Boundary averages
#' @description Computes the mean sensor value in the hour before cleaning
#'   (pre_val / drifted) and the mean of the first 4 readings after cleaning
#'   (post_val / clean) for use as correction boundaries.

cal_range_values <- function(sensor_df_arg, end_dt) {
  
  from_val <- sensor_df_arg %>%
    filter(
      !is.na(mean_analysis),
      between(DT_round, end_dt - hours(1), end_dt)
    ) %>%
    pull(mean_analysis) %>%
    mean()
  
  to_val <- sensor_df_arg %>%
    filter(
      !is.na(mean_analysis),
      DT_round > end_dt
    ) %>%
    slice_head(n = 4) %>%
    pull(mean_analysis) %>%
    mean()
  
  return(c(from_val, to_val))
}
```

```{r}
#' @title Fit exponential rate parameter k for drift correction
#' @description Normalizes the observed sensor values in the drift window to a
#'   0-1 fraction of the total drift, then fits the exponential weight formula
#'   to find the k that best describes how drift accumulated over time. Falls
#'   back to k = 1 (linear-equivalent) on failure.
#' @param df Prepped drift dataframe with columns: wt, pre_val, post_val, and correction_col
#' @param correction_col Column name of observed sensor values to fit against

exp_k <- function(df, correction_col = "mean_analysis") {
  
  pre_val  <- df[["pre_val"]][[1]]
  post_val <- df[["post_val"]][[nrow(df)]]
  
  if (is.na(pre_val) || is.na(post_val) || pre_val == post_val) return(1)
  
  fit_data <- df %>%
    filter(!is.na(.data[[correction_col]])) %>%
    mutate(observed_frac = (.data[[correction_col]] - post_val) / (pre_val - post_val)) %>%
    filter(is.finite(observed_frac)) %>%
    select(wt, observed_frac)
  
  if (nrow(fit_data) < 3) return(1)
  
  k <- tryCatch({
    fit <- nls(
      observed_frac ~ (exp(k * wt) - 1) / (exp(k) - 1),
      data    = fit_data,
      start   = list(k = 1),
      control = nls.control(maxiter = 100, warnOnly = TRUE)
    )
    coef(fit)[["k"]]
  }, error = function(e) {
    message("exp_k: nls fit failed, defaulting to k = 1. Error: ", conditionMessage(e))
    1
  })
  
  return(k)
}
```

```{r}
#' @title Unflagging function
#' @description Removes "drift" from user_flag and sets drift = FALSE for all
#'   rows falling within the time windows defined in decision_df_arg.

unflag_fxn <- function(decision_df_arg, sensor_df_arg) {
  
  if (nrow(decision_df_arg) == 0) return(sensor_df_arg)
  
  in_unflag_window <- map2(
    .x = pull(decision_df_arg, start_dt),
    .y = pull(decision_df_arg, end_dt),
    function(start, end) between(sensor_df_arg$DT_round, start, end)
  ) %>%
    reduce(`|`)
  
  remove_drift_token <- function(flag_str) {
    if (is.na(flag_str)) return(NA_character_)
    tokens <- str_split(flag_str, ";")[[1]] %>%
      str_replace_all("\\n", " ") %>%
      str_trim()
    cleaned <- tokens[tokens != "drift" & tokens != ""]
    if (length(cleaned) == 0) return(NA_character_)
    paste(cleaned, collapse = ";")
  }
  
  corrected_df <- sensor_df_arg %>%
    mutate(
      user_flag = if_else(
        in_unflag_window,
        map_chr(user_flag, remove_drift_token),
        user_flag
      ),
      drift = if_else(in_unflag_window, FALSE, drift)
    )
  
  return(corrected_df)
}
```

```{r}
#' @title Linear drift correction function
#' @description Applies a linearly-weighted single-point drift correction across
#'   each calibration window defined in decision_df_arg.

linear_correction_fxn <- function(decision_df_arg, sensor_df_arg, correction_col = "mean_analysis"){
  
  start_dt_values <- pull(decision_df_arg, start_dt)
  end_dt_values   <- pull(decision_df_arg, end_dt)
  
  data_to_correct <- map2(
    .x = start_dt_values,
    .y = end_dt_values,
    function(start, end) {
      sensor_df_arg %>%
        filter(between(DT_round, start, end))
    }
  )
  
  correction_values <- map(
    .x = end_dt_values,
    function(end_dt){
      cal_range_values(sensor_df_arg = sensor_df_arg, end_dt = end_dt)
    }
  )
  
  drift_type_values      <- pull(decision_df_arg, arg_drift_type)
  correction_type_values <- pull(decision_df_arg, arg_correction_type)
  
  corrected_data <- pmap(
    .l = list(data_to_correct, drift_type_values, correction_type_values, correction_values),
    function(df, drift, correction, corr_values) {
      
      prepped_df <- df %>%
        cal_wt(dt_col = "DT_round") %>%
        mutate(
          pre_val  = corr_values[1],
          post_val = corr_values[2]
        )
      
      corrected_df <- cal_exp_one_point_drift(
        df             = prepped_df,
        lm_trans_col   = correction_col,
        pre_col        = "pre_val",
        post_col       = "post_val",
        wt_col         = "wt",
        drift_type     = drift,
        correction_type = correction
      )
      
      return(corrected_df)
    }
  ) %>%
    bind_rows() %>%
    select(DT_round, wt, pre_val, post_val, mean_drift_trans)
  
  final_data <- sensor_df_arg %>%
    left_join(corrected_data, by = "DT_round") %>%
    mutate(mean_drift_trans = ifelse(is.na(mean_drift_trans), !!sym(correction_col), mean_drift_trans))
  
  return(final_data)
}
```

```{r}
#' @title Exponential drift correction function
#' @description Same structure as linear_correction_fxn but fits k per
#'   calibration window via exp_k() and passes it to cal_exp_one_point_drift().
#'   Fits k via exp_k() per calibration window, then applies the
#'   log weight — the transpose of the exponential curve with the same k:
#'   wt_log = log(1 + wt * (exp(k) - 1)) / k. This gives a concave weight that
#'   accumulates correction faster early and decelerates toward the end.

exponential_correction_fxn <- function(decision_df_arg, sensor_df_arg, correction_col = "mean_analysis") {
  
  start_dt_values <- pull(decision_df_arg, start_dt)
  end_dt_values   <- pull(decision_df_arg, end_dt)
  
  data_to_correct <- map2(
    .x = start_dt_values,
    .y = end_dt_values,
    function(start, end) {
      sensor_df_arg %>%
        filter(between(DT_round, start, end))
    }
  )
  
  correction_values <- map(
    .x = end_dt_values,
    function(end_dt) {
      cal_range_values(sensor_df_arg = sensor_df_arg, end_dt = end_dt)
    }
  )
  
  correction_type_values <- pull(decision_df_arg, arg_correction_type)
  
  corrected_data <- pmap(
    .l = list(data_to_correct, correction_type_values, correction_values),
    function(df, correction, corr_values) {
      
      prepped_df <- df %>%
        cal_wt(dt_col = "DT_round") %>%
        mutate(
          pre_val  = corr_values[1],
          post_val = corr_values[2]
        )
      
      k <- exp_k(prepped_df, correction_col = correction_col)
      
      corrected_df <- cal_exp_one_point_drift(
        df              = prepped_df,
        lm_trans_col    = correction_col,
        pre_col         = "pre_val",
        post_col        = "post_val",
        wt_col          = "wt",
        drift_type      = "log",
        correction_type = correction,
        exp_coef        = k
      )
      
      return(corrected_df)
    }
  ) %>%
    bind_rows() %>%
    select(DT_round, wt, pre_val, post_val, mean_drift_trans)
  
  final_data <- sensor_df_arg %>%
    left_join(corrected_data, by = "DT_round") %>%
    mutate(mean_drift_trans = ifelse(is.na(mean_drift_trans), !!sym(correction_col), mean_drift_trans))
  
  return(final_data)
}
```

# Load and prepare data

```{r}
sensor_f <- list.files(here("data", "raw", "sensor", "manual_data_verification",
                            "2025_cycle", "in_progress", "verified_directory"),
                       pattern = "Turbidity|FDOM",
                       ignore.case = TRUE,
                       full.names = TRUE)

raw_data <- map(sensor_f, arrow::read_parquet) %>%
  set_names(str_extract(basename(sensor_f), "^[^_]+"))

rm(sensor_f)
```

```{r}
field_notes <- grab_mWater_sensor_notes(load_mWater())
# TODO: fix mst thing

prepped_data <- raw_data %>%
  map(function(df) {
    df %>%
      mutate(
        drift          = ifelse(stringr::str_detect(user_flag, "drift"), TRUE, FALSE),
        drift          = ifelse(is.na(drift), FALSE, drift),
        mean_analysis  = ifelse(verification_status %in% c("FLAGGED", "PASS"), mean, NA)
      )
  })

rm(raw_data)
gc()
```

```{r}
decision_df <- read_excel(here("data", "derived", "auto_qaqc_files",
                               "drift_corrections", "2025_drift_corrections.xlsx")) %>%
  fix_site_names() 

# Split decisions into the three correction categories
unflag_df      <- decision_df %>% filter(decision == "unflag")
linear_df      <- decision_df %>% filter(arg_drift_type == "linear")
exponential_df <- decision_df %>% filter(arg_drift_type == "exponential")
```

# Examples

Use `plot_drift()` to explore a site before deciding on a correction approach.

```{r}
plot_drift(prepped_data$`archery-Turbidity`, prepped_data, site_order)
```

## Linear correction

```{r}
test_linear_decision_df <- linear_df %>%
  filter(
    site      == "cottonwood",
    parameter == "Turbidity"
  )

test_linear_sensor_df <- prepped_data$`cottonwood-Turbidity`

test_linear_result <- linear_correction_fxn(
  decision_df_arg = test_linear_decision_df,
  sensor_df_arg   = test_linear_sensor_df
)

test_linear_low_pass <- test_linear_result %>%
  low_pass_filter() %>%
  apply_interpolation_missing_data()

plot_correct_drift(test_linear_low_pass, prepped_data, site_order)
```

## Exponential correction

```{r}
test_exp_decision_df <- exponential_df %>%
  filter(
    
    site      == "cottonwood",
    parameter == "Turbidity"
  )

test_exp_sensor_df <- prepped_data$`cottonwood-Turbidity`

test_exp_result <- exponential_correction_fxn(
  decision_df_arg = test_exp_decision_df,
  sensor_df_arg   = test_exp_sensor_df
)

test_exp_low_pass <- test_exp_result %>%
  low_pass_filter() %>%
  apply_interpolation_missing_data()

plot_correct_drift(test_exp_low_pass, prepped_data, site_order)
```

## Unflag

```{r}
test_unflag_decision_df <- unflag_df %>%
  filter(
    site      == "cottonwood",
    parameter == "Turbidity"
  )

test_unflag_sensor_df <- prepped_data$`cottonwood-Turbidity`

test_unflag_result <- unflag_fxn(
  decision_df_arg = test_unflag_decision_df,
  sensor_df_arg   = test_unflag_sensor_df
)

plot_unflag_comparison(test_unflag_sensor_df, test_unflag_result)
```

# The whole workflow orchestrated

```{r}
# For each site-parameter, apply all corrections in order then stitch together.
# Exponential decisions use the log correction (the functional inverse of the
# exponential weight), which better matches how biofouling drift accumulates.

corrected_data <- prepped_data %>%
  imap(function(df, key) {

    site_val      <- unique(df$site)
    parameter_val <- unique(df$parameter)
    
    # Filter decisions for this site-parameter
    site_decisions <- decision_df %>%
      filter(site == site_val, parameter == parameter_val)

    # 1. Unflag: update drift flags and user_flag (does not change mean_analysis)
    df_working <- unflag_fxn(
      decision_df_arg = site_decisions %>% filter(decision == "unflag"),
      sensor_df_arg   = df
    )

    # Helper: boolean vector marking rows inside any window in a decision subset
    in_windows <- function(dec_df) {
      if (nrow(dec_df) == 0) return(rep(FALSE, nrow(df_working)))
      map2(
        pull(dec_df, start_dt),
        pull(dec_df, end_dt),
        function(s, e) between(df_working$DT_round, s, e)
      ) %>% reduce(`|`)
    }

    # 2. Identify which rows belong to each correction window type
    unflag_decision_df <- site_decisions %>% filter(decision == "unflag")
    linear_decision_df <- site_decisions %>% filter(arg_drift_type == "linear")
    exp_decision_df    <- site_decisions %>% filter(arg_drift_type == "exponential")

    in_unflag <- in_windows(unflag_decision_df)
    in_linear <- in_windows(linear_decision_df)
    in_exp    <- in_windows(exp_decision_df)

    # 3. Compute corrections and join results back onto df_working by DT_round.
    #    distinct() guards against row expansion from overlapping decision windows.
    if (any(in_linear)) {
      linear_join <- linear_correction_fxn(
        decision_df_arg = linear_decision_df,
        sensor_df_arg   = df_working
      ) %>%
        distinct(DT_round, .keep_all = TRUE) %>%
        select(DT_round, linear_trans = mean_drift_trans)
      df_working <- df_working %>% left_join(linear_join, by = "DT_round")
    } else {
      df_working <- df_working %>% mutate(linear_trans = NA_real_)
    }

    if (any(in_exp)) {
      exp_join <- exponential_correction_fxn(
        decision_df_arg = exp_decision_df,
        sensor_df_arg   = df_working %>% select(-any_of("linear_trans"))
      ) %>%
        distinct(DT_round, .keep_all = TRUE) %>%
        select(DT_round, exp_trans = mean_drift_trans)
      df_working <- df_working %>% left_join(exp_join, by = "DT_round")
    } else {
      df_working <- df_working %>% mutate(exp_trans = NA_real_)
    }

    # 4. Stitch: assign corrected values and stamp correction_type for plotting
    df_corrected <- df_working %>%
      mutate(
        mean_drift_trans = case_when(
          in_linear ~ linear_trans,
          in_exp    ~ exp_trans,
          TRUE      ~ mean_analysis
        ),
        correction_type = case_when(
          in_linear ~ "linear",
          in_exp    ~ "exponential",
          in_unflag ~ "unflagged",
          TRUE      ~ "raw"
        )
      ) %>%
      select(-linear_trans, -exp_trans)

    # 5. Post-process: smooth (Turbidity only) then interpolate short gaps
    df_corrected %>%
      apply_interpolation_missing_data(value_col = "mean_drift_trans", new_value_col = "mean_filled")%>%
       low_pass_filter(correction_col = "mean_filled")
  })
```

```{r}
# Save each site-parameter data frame as a parquet file.
# File names match the list keys with spaces replaced by underscores.

save_dir <- here("data", "raw", "sensor", "manual_data_verification",
                 "2025_cycle", "post_verification")

iwalk(corrected_data, function(df, key) {
  filename  <- paste0(str_replace_all(key, " ", "_"), ".parquet")
  arrow::write_parquet(df, file.path(save_dir, filename))
})
```

After all of this work, the final column to consider is: `mean_filled`

# Plotting the final corrections

Explore the final results here.

```{r}
plot_final_corrections(
  df                  = fdom_data$`udall-FDOM Fluorescence`,
  data_list           = prepped_data,
  site_order_template = site_order
)
```
