---
title: "Organizing raw data"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 90
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE) 
```

Load necessary packages:

```{r}
source("src/package_loader.R")
lapply(c("data.table", "tidyverse", "rvest", "readxl", "lubridate", "zoo", "padr","plotly", "feather", "RcppRoll", "yaml", "ggpubr", "profvis", "janitor"), package_loader)
sapply(list.files("src/qaqc/flagging_functions", pattern = "*.R", full.names = TRUE), source, .GlobalEnv)
sapply(list.files("src/qaqc/plotting_functions", pattern = "*.R", full.names = TRUE), source, .GlobalEnv)
sapply(list.files("src/qaqc/data_munging_functions", pattern = "*.R", full.names = TRUE), source, .GlobalEnv)
sapply(list.files("src/qaqc/verification_functions", pattern = "*.R", full.names = TRUE), source, .GlobalEnv)
```

# Import and collate data

Load field notes and define the start time as the 15 minute preceding the field time
```{r}
field_notes <- clean_field_notes()
```

Merge the data sets from all API pulls:
```{r}
all_data <- munge_api_data()
```

### Export collated raw file

```{r}
# This will be a parquet file in the future?
#write_feather(all_data, paste0('data/SOME_FOLDER_FOR_POSTERITY/collated_raw_sonde_v', Sys.Date(), '.feather'))
```

# Level 1 QA-QC

### Temperature at Archery

I think we will want to develop pipelines specific for each site and parameter for
processing speed (targets?). Here I am starting a pipeline for Archery's temperature:

#### Format data
```{r}
# Determine each site and parameter in all_data 
sites <- unique(all_data$site)
params <- c("Actual Conductivity", "Battery Level", "Baro", "Chl-a Fluorescence", 
            "Depth", "DO", "External Voltage", "FDOM Fluorescence", "ORP", 
            "pH", "Specific Conductivity", "Temperature", "Turbidity")
 
# Constructing a df to iterate over each site-parameter combination
combinations <- crossing(sites, params)

# Make a list of the summarized data
all_data_summary_list <- map2(combinations$sites, 
                         combinations$params, 
                         summarize_site_param,
                         api_data = all_data) %>% 
  # set the names for the dfs in the list
  set_names(paste0(combinations$sites, "-", combinations$params)) %>% 
  # remove NULL values from the list
  keep(~ !is.null(.))

# Bind rows for each df in list
# all_data_summary_df <- bind_rows(all_data_summary_list)
all_data_summary_df <- bind_rows(all_data_summary_list)
```

#### Add summary statistics to the formatted data
```{r}
all_data_summary_stats_list <- map(all_data_summary_list, generate_summary_statistics)
```

#### Add flagging functions
Add flagging functions for each df in all_data_summary_list

Pass the dfs in all_data_summary_stats_list through the flagging functions
```{r}
all_data_flagged <- map(all_data_summary_stats_list, add_all_flags)
```

#### Add verification column
```{r}
all_data_flagged <- map(all_data_flagged, add_verification_column)
```

#### Visualize the flags

```{r}
# Current vis:
plotly_plot <- plotly::ggplotly(ggplot() +
  geom_point(data = filter(all_data_flagged[["archery-Temperature"]], is.na(flag)), 
             aes(x=DT_round, y = mean)) +
  geom_point(data = filter(all_data_flagged[["archery-Temperature"]], !is.na(flag)), 
             aes(x=DT_round, y = mean, color = flag)) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  facet_wrap(~year))
```

Visualizing daily plots
```{r, fig.width=12, fig.height=7}
generate_daily_flag_plots("lincoln", "Chl-a Fluorescence", "outside sd range")
```

Visualizing weekly plots
```{r, fig.width=12, fig.height=7}
#generate_weekly_flag_plots("archery", "Temperature", "slope violation")
```

Stack the weekly and daily plots
```{r, fig.width=12, fig.height=12}
# stack_flag_plots("boxelder", "Turbidity", "outside sd range")
```

Histograms of all parameter/site combos
```{r}
# making histograms for temperatures
## pulling all the temperature dfs from the all_data list

# temperature_hist_list <- all_data_flagged[grep("Temperature", names(all_data_summary_list), ignore.case = TRUE)]

## use imap to use the index in the list for the plot titles
## (not loving imap, if there is a better way to do this please change this)

# temperature_hist_plots <- imap(temperature_hist_list, ~generate_general_histogram(.x, .y))
# temperature_hist_plots

## How to pull the histogram data that ggplot generates to make histograms
# temperature_hist_data <- map(temperature_hist_plots, ~ ggplot_build(.x)$data[[1]]) 
```

```{r}
# imap(all_data_flagged, ~generate_seasonal_histogram(.x, .y))

# output_dir <- "/Users/juandlt_csu/Library/CloudStorage/OneDrive-Colostate/poudre_sonde_network/data/histogram_images"
# 
# # Use imap to iterate over the list and generate/save plots
# imap(all_data_flagged, ~{
#   df_name <- .y  # Get the name of the data frame
#   plot <- generate_seasonal_histogram(.x, .y)  # Generate the plot
# 
#   # Create a filename based on the name of the data frame
#   filename <- file.path(output_dir, paste0(df_name, ".png"))
# 
#   # Save the plot as an image (you can adjust the file format and options)
#   ggsave(filename, plot, device = "png", width = 8, height = 6)
# })
```
